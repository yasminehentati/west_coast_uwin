################################################################################
################# West Coast Env Health GLMM Analysis ##########################
#################         Step 2: Covariates          ##########################
#################   Yasmine Hentati yhentati@uw.edu   ##########################
################################################################################

### NDVI calculation code adapted from T. Gallo ###
### NDVI composite dataset generated by G. Cova ### 

## packages 
library(raster)
library(sf)
library(dplyr)
library(mapview) # note that mapview is somewhat demanding, consider skipping
# mapview functions if your computer is slow  - these are just used to check 
# that shapefiles look correct 
library(rgdal)
library(raster)
library(here)
library(remotes)
library(tidycensus)
# remotes::install_github("walkerke/crsuggest") 
library(crsuggest)
library(tidyr)
library(terra)
install.packages("spatialEco")
library(spatialEco)
library(readr)
library(ggfortify)
library(rgeos)

################################################################################
## notes on projections: we're going to use WGS84/UTM because we need to work in 
## meters for our buffers. however, the data is split across 2 zones (11N and 10N)
## so all steps will need to be done for each zone 
################################################################################
## CAMERA LOCATIONS 
## get camera points 

# load all sites
all_sites <- read.csv(here("data", "counts_cleaned.csv"), stringsAsFactors = FALSE)

# order data by Location Name 
all_sites <- all_sites[order(all_sites$Site),]
nrow(all_sites)

# split by zone

# create new column for utm zone 
data_10_WA <- all_sites %>% subset(City == "tawa")
data_10_WA$utmZone <- "10"

data_10_SF <- all_sites %>% subset(City == "oaca")
data_10_SF$utmZone <- "10"

data_LA <- all_sites %>% subset(City == "paca" | City == "lbca")
data_LA$utmZone <- "11"

# transform into UTM - WA
utm1 <- data.frame(x=data_10_WA$Long, y=data_10_WA$Lat) 
coordinates(utm1) <- ~x+y 
proj4string(utm1) <- CRS("+proj=longlat +datum=WGS84") 
utm2 <- spTransform(utm1,CRS("+init=epsg:32610"))

data_10_WA$utmEast <- utm2$x
data_10_WA$utmNorth <- utm2$y

# transform into UTM - SF
utm1 <- data.frame(x=data_10_SF$Long, y=data_10_SF$Lat) 
coordinates(utm1) <- ~x+y 
proj4string(utm1) <- CRS("+proj=longlat +datum=WGS84") 
utm2 <- spTransform(utm1,CRS("+init=epsg:32610"))

data_10_SF$utmEast <- utm2$x
data_10_SF$utmNorth <- utm2$y

# transform into UTM - LA
utm1 <- data.frame(x=data_LA$Long, y=data_LA$Lat) 
coordinates(utm1) <- ~x+y 
proj4string(utm1) <- CRS("+proj=longlat +datum=WGS84") 
utm2 <- spTransform(utm1,CRS("+init=epsg:32611"))

data_LA$utmEast <- utm2$x
data_LA$utmNorth <- utm2$y

# turn coordinates into spatial points
points_WA <- data_10_WA %>% distinct(Site, .keep_all = TRUE) %>%
  st_as_sf(coords = c("utmEast", "utmNorth"), crs = 
                     32610)
points_WA
points_SF <- st_as_sf(data_10_SF, coords = c("utmEast", "utmNorth"), crs = 
                        32610)

points_LA <- st_as_sf(data_LA, coords = c("utmEast", "utmNorth"), crs = 
                        32611)


# check that the points look ok
# mapview(points_WA)
# mapview(points_SF)
# mapview(points_LA)


# keep only 1 unique site row for each 

points_WA <- points_WA %>% distinct(Site, .keep_all = TRUE)
points_WA$Site

points_SF <- points_SF %>% distinct(Site, .keep_all = TRUE)
points_SF

points_LA <- points_LA %>% distinct(Site, .keep_all = TRUE)
points_LA

################################################################################
## INCOME DATA
## median household income data from American Community Survey

Sys.getenv("CENSUS_API_KEY")
# load census API key
# census_api_key("e649b78a1d98fe7e1c9ff7039840781976777eb6",
           #    install = TRUE)
# readRenviron("~/.Renviron")

# load in ACS variables  from 2019 
v17 <- load_variables(2019, "acs5", cache = TRUE)


# load in shapefile of subdivisions  
# with median household income as variable of interest

# for washington 
tractincomeWA <- get_acs(state = "WA", 
                         geography = "tract", 
                         variables = c(medincome="B19013_001"), geometry = TRUE)

# for california
tractincomeCA <- get_acs(state = "CA", 
                         geography = "tract", 
                         variables = c(medincome="B19013_001"), geometry = TRUE)

# save whole state to a shapefile - currently not needed so will leave as comment
# st_write(tractincomeWA, "WA_med_income.shp", append = FALSE)
# st_write(tractincomeCA, "CA_med_income.shp", append = FALSE)

# determine coordinate system
suggest_top_crs(tractincomeWA) # 32148 is projected CRS 

# cut out everything except king and pierce county from our WA shp 
# specify dplyr because rgdal also has filter fxn 
# do this using the GEOID codes for each county 

tractsKP <- tractincomeWA %>% dplyr::filter(substr(GEOID, 1, 5) 
                                            %in% c("53033", "53053"))

# mapview(tractsKP) # looks good

# write king and pierce county shapefiles 

# st_write(tractsKP, here("data", "income_maps", "seatac_med_income.shp"),
    #      append = FALSE)

# crop to actual study area 
tractsKP <- st_transform(tractsKP, crs=4326)
tractsKP_crop <- st_crop(tractsKP, c(xmin= -121.7, ymin = 46.7, xmax = -122.8, ymax = 47.8))

# write shapefiles for cropped area 
# st_write(tractsKP_crop, here("data", "income_maps", "seatac_urban_med_income.shp"),
     #     append = FALSE)


# do the same for the bay area
# counties: napa marin solano contra costa 
# alameda san fransisco san mateo santa clara 

tractsSF <- tractincomeCA %>% dplyr::filter(substr(GEOID, 1, 5) 
                                            %in% c("06055", "06041", 
                                                   "06095", "06013",
                                                   "06001", "06075",
                                                   "06081", "06085"))

# mapview(tractsSF)

# write SF bay area  shapefiles 
# st_write(tractsSF, here("data", "income_maps", "sf_bay_med_income.shp"),
         # append = FALSE)

# do the same for LA
# just LA county and orange county
tractsLA <- tractincomeCA %>% dplyr::filter(substr(GEOID, 1, 5) 
                                            %in% c("06037", "06059"))
colnames(tractsLA)
# mapview(tractsLA)
# write LA area shapefiles 
# st_write(tractsLA, here("data", "income_maps", "la_orange_county_med_income.shp"),
     #      append = FALSE)


################################################################################
## HOUSING DENSITY DATA 
## read in housing density  data - starting with WA 

wa_housing <- st_read(here("data", "housing_maps", 
        "wa_blk10_Census_change_1990_2010_PLA2.shp"))

# filter to only king and pierce county
wa_housing <- wa_housing %>% dplyr::filter(substr(BLK10, 1, 5) 
                                            %in% c("53033", "53053"))

# we only need 2010 housing data - select relevant info

colnames(wa_housing)
wa_housing <- wa_housing %>% dplyr::select(BLK10, WATER10, POP10, 
                                           HU10, HUDEN10, HHUDEN10,
                                           PUBFLAG:geometry)
colnames(wa_housing)

# crop to actual study area 
sf_use_s2(FALSE)
wa_housing <- st_transform(wa_housing, crs=4326)
wa_housing <- st_crop(wa_housing, c(xmin= -121.7, ymin = 46.7, xmax = -122.8, ymax = 47.8))

# mapview(wa_housing)

# st_write(wa_housing, here("data", "housing_maps", "wa_urban_huden_2010.shp"),
      #    append = FALSE)

# do the same with SF 

ca_housing <- st_read(here("data", "housing_maps", 
                           "ca_blk10_Census_change_1990_2010_PLA2.shp"))

# filter to only bay area counties
sf_housing <- ca_housing %>% dplyr::filter(substr(BLK10, 1, 5) 
                                           %in% c("06055", "06041", 
                                                  "06095", "06013",
                                                  "06001", "06075",
                                                  "06081", "06085"))

# we only need 2010 housing data - select relevant info

colnames(sf_housing)
sf_housing <- sf_housing %>% dplyr::select(BLK10, WATER10, POP10, 
                                           HU10, HUDEN10, HHUDEN10,
                                           PUBFLAG:geometry)
colnames(sf_housing)


# st_write(sf_housing, here("data", "housing_maps", "sf_urban_huden_2010.shp"),
     #     append = FALSE)



# do the same with LA 

# filter to only LA cty and orange cty
la_housing <- ca_housing %>% dplyr::filter(substr(BLK10, 1, 5) 
                                           %in% c("06037", "06059"))

# we only need 2010 housing data - select relevant info

colnames(la_housing)
la_housing <- la_housing %>% dplyr::select(BLK10, WATER10, POP10, 
                                           HU10, HUDEN10, HHUDEN10,
                                           PUBFLAG:geometry)

colnames(la_housing)

# st_write(la_housing, here("data", "housing_maps", "la_urban_huden_2010.shp"),
     #      append = FALSE)



################################################################################
## VEGETATION DATA 

# load LandSat NDVI raster from GRC GEE code 

ndvi_kp <- raster(here("data", "NDVI_data", "NDVI2020-TAWA-30-3857.TIF"))

suggest_top_crs(ndvi_kp)
ndvi_kp <- projectRaster(ndvi_kp, crs = "EPSG:6599")

# reproject points to raster crs 
points_WA <- st_transform(points_WA, crs = st_crs(ndvi_kp))
points_WA
# extract the proportion of the buffer that has an NDVI greater than 0.2 (vegetation cover)
# this returns a list, so we can use lapply to calculate the proportion for each site
ndvi_extract <- raster::extract(ndvi_kp, points_WA, buffer = 1000)

# calculate the proportion of a site that is covered in vegetation
prop_ndvi_greater0.2 <- lapply(ndvi_extract, function (x){
  # turn values greater than 0.2 to 1 (these are cells that are covered in vegetation)
  x[which(x > 0.2)] <- 1
  # turn values less than 0.2 to 0 (these are cells that are not vegetation)
  x[which(x <= 0.2)] <- 0
  # proportion of cells that are vegetation
  sum(x, na.rm = TRUE)/length(x) })


# turn list into a vector
prop_veg <- do.call(c, prop_ndvi_greater0.2)
prop_veg

points_WA$prop_veg <- prop_veg

# for SF
# currently this doesn't cover the whole bay area - will need to update 

ndvi_sf <- raster(here("data", "NDVI_data", "NDVI2020_OACA-30-3857.TIF"))
suggest_top_crs(ndvi_sf)
ndvi_sf <- projectRaster(ndvi_sf, crs = "EPSG:7132")

# reproject points to raster crs 
points_SF <- st_transform(points_SF, crs = st_crs(ndvi_sf))
points_SF

# extract the proportion of the buffer that has an NDVI greater than 0.2 (vegetation cover)
# this returns a list, so we can use lapply to calculate the proportion for each site
ndvi_extract <- raster::extract(ndvi_sf, points_SF, buffer = 1000)

# calculate the proportion of a site that is covered in vegetation
prop_ndvi_greater0.2 <- lapply(ndvi_extract, function (x){
  # turn values greater than 0.2 to 1 (these are cells that are covered in vegetation)
  x[which(x > 0.2)] <- 1
  # turn values less than 0.2 to 0 (these are cells that are not vegetation)
  x[which(x <= 0.2)] <- 0
  # proportion of cells that are vegetation
  sum(x, na.rm = TRUE)/length(x) })


# turn list into a vector
prop_veg <- do.call(c, prop_ndvi_greater0.2)
prop_veg

points_SF$prop_veg <- prop_veg

# for LA 

# read in NDVI
ndvi_la <- raster(here("data", "NDVI_data", "NDVI2020_LACA-30-3857.TIF"))
suggest_top_crs(ndvi_la)
ndvi_la <- projectRaster(ndvi_la, crs = "EPSG:26799")

# reproject points to raster crs 
points_LA <- st_transform(points_LA, crs = st_crs(ndvi_la))
st_crs(points_LA)
# extract the proportion of the buffer that has an NDVI greater than 0.2 (vegetation cover)
# this returns a list, so we can use lapply to calculate the proportion for each site
ndvi_extract <- raster::extract(ndvi_la, points_LA, buffer = 1000)

# calculate the proportion of a site that is covered in vegetation
prop_ndvi_greater0.2 <- lapply(ndvi_extract, function (x){
  # turn values greater than 0.2 to 1 (these are cells that are covered in vegetation)
  x[which(x > 0.2)] <- 1
  # turn values less than 0.2 to 0 (these are cells that are not vegetation)
  x[which(x <= 0.2)] <- 0
  # proportion of cells that are vegetation
  sum(x, na.rm = TRUE)/length(x) })


# turn list into a vector
prop_veg <- do.call(c, prop_ndvi_greater0.2)
prop_veg

points_LA$prop_veg <- prop_veg

################################################################################
## ENVIRONMENTAL HEALTH DATA 
# obtained from calenviroscreen and washington environmental health disparities map

## starting with calenviroscreen
calenv <- read.csv(here("data", "calenviroscreen_v4.csv"))

head(calenv)

#get only our counties of interest 
calenv <- calenv %>% dplyr::filter(substr(Census.Tract, 1, 4) 
                                         %in% c("6055", "6041", 
                                                "6095", "6013",
                                                "6001", "6075",
                                                "6081", "6085",
                                                "6037", "6059"))
# remove variables we don't care about
calenv <- calenv %>% dplyr::select(Census.Tract, Pollution.Burden.Score)

# convert into rankings to match washington data

# drop NAs
calenv <- drop_na(calenv)

# round to ranking - we'll do the same as WA EHD and round at 0.5
# skipping this right now since we're doing a buffer anyway

# half_ceil <- function(x){
 #  whole = ceiling(x)
#   if(x >= whole - .5){
#     return(whole)
#   } else
 #  return(whole - 1)
# }

# calenv$CESrank <- sapply(calenv$NewPercentile, half_ceil)
calenv

# now with wa
waenv <- read.csv(here("data", "wa_envhealthdisp_v2.csv"))

#get only our counties of interest 
waenv <- waenv %>% dplyr::filter(substr(State.FIPS.Code, 1, 5) 
                                   %in% c("53053", "53033"))


# rename col names to match
colnames(waenv) <- c("GEOID", "Rank")
colnames(calenv) <- c("GEOID", "Rank")

# add leading 0s to california data
library(bit64)

# for mac 
# calenv$GEOID <- sprintf("%011f", calenv$GEOID)

# for mac: this is now characters, so change waenv to match
# waenv$GEOID <- as.character(waenv$GEOID)

# for windows - sprintf doesn't seem to work so using a different solution
calenv$GEOID <- sprintf(fmt = "%011s", calenv$GEOID) %>% 
  gsub(pattern = " ", replacement = "0", x = .)


# bind together
env_data <- rbind(waenv,calenv)
env_data$GEOID

################################################################################

################################################################################
## impervious surface calculation 
library(here)
# load impervious cover raster map
imp_map <- raster(here("data", "NLCD_imp", 
                       "nlcd_2019_impervious_descriptor_l48_20210604.img"))

# again reproject  points to match raster
# these are just the points not the buffers
points_WA <- st_transform(points_WA, st_crs(imp_map))

# extract the mean impervious cover around each point using 500 m radius buffer
imp <- raster::extract(imp_map, points_WA, fun=mean, buffer= 1000, df=TRUE)
imp

points_WA$imp_surf <- imp$nlcd_2019_impervious_descriptor_l48_20210604

# for SF 
points_SF <- st_transform(points_SF, st_crs(imp_map))

# extract the mean impervious cover around each point using 500 m radius buffer
imp <- raster::extract(imp_map, points_SF, fun=mean, buffer= 1000, df=TRUE)
imp

points_SF$imp_surf <- imp$nlcd_2019_impervious_descriptor_l48_20210604

# for LA 
points_LA <- st_transform(points_LA, st_crs(imp_map))

# extract the mean impervious cover around each point using 500 m radius buffer
imp <- raster::extract(imp_map, points_LA, fun=mean, buffer= 1000, df=TRUE)
points_LA$imp_surf <- imp$nlcd_2019_impervious_descriptor_l48_20210604

################################################################################
# additional calculations

# WA - merge the env health data to polygons

# ran into some issues with non-matching census tracts
# so we'll manually make sure all point have a matching polygon
points_WA <- st_transform(points_WA, st_crs(tractsKP))

env_WA_sp <- merge(tractsKP, env_data, by.x = "GEOID",
                   by.y = "GEOID", all.x = TRUE) 

# mapview(list(points_WA,env_WA_sp), zcol = list(NULL, "Rank"))

# get GEOID and rank for each point by finding what polygon they lie within
env_pts_WA <- point.in.poly(points_WA, env_WA_sp)

# find GEOIDs without ranks and manually enter averages of surrounding polygons
# find faster way to do this later
missingWA <- env_pts_WA[is.na(env_pts_WA$Rank), ]   # empty Ranks within points

env_data$Rank[env_data$GEOID==53053063401]
env_WA_sp$Rank[env_WA_sp$GEOID==53053063401] <-  10 # fill the polygon shapefile
# with corresponding rank for the GEOID that point of interest lies within 
# mapview(list(env_WA_sp, points_WA), zcol = list("Rank", NULL))


# LA - merge env health data to polygons 
points_LA <- st_transform(points_LA, st_crs(tractsLA))

env_LA_sp <- merge(tractsLA, env_data, by.x = "GEOID",
                   by.y = "GEOID", all.x = TRUE) 

# mapview(list(points_LA,env_LA_sp), zcol = list(NULL, "Rank"))

# get GEOID and rank for each point by finding what polygon they lie within
env_pts_LA <- point.in.poly(points_LA, env_LA_sp)

# find GEOIDs without ranks and manually enter averages of surrounding polygons
# find faster way to do this later
missingLA <- env_pts_LA[is.na(env_pts_LA$Rank), ]   # empty Ranks within points
# mapview(list(env_LA_sp, missingLA), zcol = list("Rank", NULL))


env_LA_sp$Rank[env_LA_sp$GEOID=="06037930400"] <- ((4.66+4.26+6.11+5.65+6.11)/5)   # fill the polygon shapefile
# with corresponding rank for the GEOID that point of interest lies within 
env_LA_sp$Rank[env_LA_sp$GEOID=="06037300602"] <- ((6.46+6.81+5.38+6.32+5.65)/5)
env_LA_sp$Rank[env_LA_sp$GEOID=="06059052430"] <- 4.18
env_LA_sp$Rank[env_LA_sp$GEOID=="06059052437"] <- 5.49
# mapview(list(env_LA_sp, points_LA), zcol = list("Rank", NULL))




# SF - merge env health data to polygons 
points_SF <- st_transform(points_SF, st_crs(tractsSF))

env_SF_sp <- merge(tractsSF, env_data, by.x = "GEOID",
                   by.y = "GEOID", all.x = TRUE) 

# mapview(list(points_SF,env_SF_sp), zcol = list(NULL, "Rank"))

# get GEOID and rank for each point by finding what polygon they lie within
env_pts_SF <- point.in.poly(points_SF, env_SF_sp)

# find GEOIDs without ranks and manually enter averages of surrounding polygons
# find faster way to do this later
# missingSF <- env_pts_SF[is.na(env_pts_SF$Rank), ]   # empty Ranks within points
# mapview(list(env_SF_sp, missingSF), zcol = list("Rank", NULL))
# none needed at this time 

# remove empty geometries first
env_WA_sp <- env_WA_sp[!st_is_empty(env_WA_sp),]
env_SF_sp <- env_SF_sp[!st_is_empty(env_SF_sp),]
env_LA_sp <- env_LA_sp[!st_is_empty(env_LA_sp),]

# reproject to match NDVI rasters
env_WA_sp <- st_transform(env_WA_sp, st_crs(ndvi_kp))
env_LA_sp <- st_transform(env_LA_sp, st_crs(ndvi_la))
env_SF_sp <- st_transform(env_SF_sp, st_crs(ndvi_sf))


# rasterize so we can get buffers for the points 
wa_env_rast <- rasterize(env_WA_sp, ndvi_kp,
                          field = "Rank")

sf_env_rast <- rasterize(env_SF_sp, ndvi_sf,
                         field = "Rank")

la_env_rast <- rasterize(env_LA_sp, ndvi_la,
                         field = "Rank")


# reproject points to match 
points_LA <- st_transform(points_LA, st_crs(la_env_rast))
points_WA <- st_transform(points_WA, st_crs(wa_env_rast))
points_SF <- st_transform(points_SF, st_crs(sf_env_rast))

# plot to check



# extract buffers
wa_env_values <- raster::extract(wa_env_rast, points_WA, fun=mean, buffer= 1000, df=TRUE)

sf_env_values <- raster::extract(sf_env_rast, points_SF, fun=mean, buffer= 1000, df=TRUE)

la_env_values <- raster::extract(la_env_rast, points_LA, fun=mean, buffer= 1000, df=TRUE)


points_WA$rank_buff <- wa_env_values$layer
points_SF$rank_buff <- sf_env_values$layer
points_LA$rank_buff <- la_env_values$layer



################################################################################

################################################################################

# housing density buffer - WA


#  transform to shapefile - need a masking raster, we'll use NDVI 
# transform to same proj
wa_housing <- st_transform(wa_housing, st_crs(ndvi_kp))
# change to raster 
wa_hous_rast <- rasterize(wa_housing, ndvi_kp,
                          field = "HUDEN10")

# transform points to same proj as raster
points_WA<- st_transform(points_WA, st_crs(wa_hous_rast))

# calculate housing density average w/ 1000m buffer
hu_den <- raster::extract(wa_hous_rast, points_WA, fun=mean, buffer= 1000, df=TRUE)
hu_den

points_WA$huden2010 <- hu_den$layer

# housing density buffer - SF
#  transform to shapefile - need a masking raster, we'll use NDVI 
# transform to same proj
sf_housing <- st_transform(sf_housing, st_crs(ndvi_sf))
# change to raster 
sf_hous_rast <- rasterize(sf_housing, ndvi_sf,
                          field = "HUDEN10")

# transform points to same proj as raster
points_SF<- st_transform(points_SF, st_crs(sf_hous_rast))

# calculate housing density average w/ 1000m buffer
hu_den <- raster::extract(sf_hous_rast, points_SF, fun=mean, buffer= 1000, df=TRUE)
hu_den

points_SF$huden2010 <- hu_den$layer

# housing density buffer - LA 
# for LA, we'll need to clip the shp (there are island in LA count that 
# are cut out of our raster)

# first we'll make a polygon out of our raster extent to create a boundary,
# then clip the housing density polygon

# we're also going to clip the NDVI file because it's huge


ndvi_poly <- as.polygons(ext(ndvi_la), crs = "EPSG:26799")
ndvi_poly <- st_as_sf(ndvi_poly)


# transform to same proj
la_housing <- st_transform(la_housing, st_crs(ndvi_poly))
la_hous_crop <- st_intersection(la_housing, ndvi_poly)


#  transform to shapefile - need a masking raster 

# change to raster 
la_hous_rast <- rasterize(la_hous_crop, ndvi_la,
                          field = "HUDEN10")

# transform points to same proj as raster
points_LA<- st_transform(points_LA, st_crs(la_hous_rast))

# calculate housing density average w/ 1000m buffer
hu_den <- raster::extract(la_hous_rast, points_LA, fun=mean, buffer= 1000, df=TRUE)
hu_den
points_LA$huden2010 <- hu_den$layer
points_WA$huden2010
points_LA$huden2010
################################################################################
# income buffer - WA



#  transform to shapefile - need a masking raster, we'll use NDVI 
# transform to same proj
wa_income <- st_transform(tractsKP, st_crs(ndvi_kp))
glimpse(wa_income)

# remove empty geometries first
wa_income <- wa_income[!st_is_empty(wa_income),]

# change to raster 
wa_inc_rast <- rasterize(wa_income, ndvi_kp,
                          field = "estimate")

# transform points to same proj as raster
points_WA<- st_transform(points_WA, st_crs(wa_inc_rast))

# calculate housing density average w/ 1000m buffer
med_inc <- raster::extract(wa_inc_rast, points_WA, fun=mean, buffer= 1000, df=TRUE)
med_inc

points_WA$med_inc <- med_inc$layer

# income buffer - SF


#  transform to shapefile - need a masking raster, we'll use NDVI 
# transform to same proj
sf_income <- st_transform(tractsSF, st_crs(ndvi_sf))

# remove empty geometries first
sf_income <- sf_income[!st_is_empty(sf_income),]

# change to raster 
sf_inc_rast <- rasterize(sf_income, ndvi_sf,
                         field = "estimate")

# transform points to same proj as raster
points_SF<- st_transform(points_SF, st_crs(sf_inc_rast))

# calculate housing density average w/ 1000m buffer
med_inc <- raster::extract(sf_inc_rast, points_SF, fun=mean, buffer= 1000, df=TRUE)
med_inc

points_SF$med_inc <- med_inc$layer

# income buffer - LA 
#  transform to shapefile - need a masking raster, we'll use NDVI 
# transform to same proj
la_income <- st_transform(tractsLA, st_crs(ndvi_la))

# remove empty geometries first
la_income <- la_income[!st_is_empty(la_income),]

# change to raster 
la_inc_rast <- rasterize(la_income, ndvi_la,
                         field = "estimate")

# transform points to same proj as raster
points_LA<- st_transform(points_LA, st_crs(la_inc_rast))

# calculate housing density average w/ 1000m buffer
med_inc <- raster::extract(la_inc_rast, points_LA, fun=mean, buffer= 1000, df=TRUE)
med_inc

points_LA$med_inc <- med_inc$layer

# merge WA/SF/LA points 
suggest_top_crs(points_LA)

glimpse(points_LA)
glimpse(points_SF)
# first get everything into the same projection 
points_WA <- st_transform(tractsKP, st_crs(ndvi_kp))
wa_income <- st_transform(tractsKP, st_crs(ndvi_kp))
wa_income <- st_transform(tractsKP, st_crs(ndvi_kp))

head(points_WA)


################################################################################


# pca
# don't need geometries
data_WA <- st_drop_geometry(points_WA)
data_SF <- st_drop_geometry(points_SF)
data_LA <- st_drop_geometry(points_LA)

# bind all together
all_CA <- rbind(data_SF, data_LA)
all_dat <- rbind(all_CA, data_WA)

colnames(all_dat)
urb_pca <- prcomp(all_dat[,c(30,31,33)], center = TRUE,scale. = TRUE)


summary(urb_pca)
urb_pca$rotation

# eigenvectors in r are default negative - flip to positive 
urb_pca$rotation <- -urb_pca$rotation

# plot pca
biplot(urb_pca, scale = 0)

# calculate var explained by each pc
(VE <- urb_pca$sdev^2)

# calculate proportion of variance explained by each pc
PVE <- VE / sum(VE)
round(PVE, 2)

# pc1 explains 52% of variance aso we'll use that as our urbanization pca
urb_pca_all <- urb_pca$x 
urb_pca_all <- as.data.frame(urb_pca_all)
all_dat$urb_pca <- urb_pca_all$PC1

# let's make some nicer plots to visualize
autoplot(urb_pca, data = all_dat, colour = 'City', loadings = TRUE, scale = 0)


################################################################################
# add back in the rest of our count data
colnames(all_dat)
colnames(all_sites)

# merge only covariate columns
all_sites_covs <- all_sites %>% merge(all_dat[,c(1,29:35)], by = "Site", 
                    keep_all_x = TRUE)

all_sites_covs %>% group_by(City) 

# write csv to save all covs 
write_csv(all_sites_covs, here("data", "all_data_counts_covs_10-27-22.csv"))

# add in covariates to richness and diversity data 

vegandf <- read_csv("vegan_sites.csv")
glimpse(vegandf)

all_sites_covs_vegan <- all_sites %>% merge(all_dat[,c(1,29:35)], by = "Site", 
                                      keep_all_x = TRUE)


# write csv to save all covs 
write_csv(all_sites_covs_vegan, here("data", "all_data_vegan_covs_10-27-22.csv"))
